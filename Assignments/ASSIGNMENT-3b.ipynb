{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3b: Writing your own nlp program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this part of the assignment, we will carry out our own little text analysis project. The goal is to gain some insights into a collection of novels without having to read them all. \n",
    "\n",
    "Specifically, we will compare most frequent words and perform some sentiment analysis.\n",
    "\n",
    "Also, we will make sure to split up tasks into smaller tasks and test our code on small toy examples before moving to bigger texts. \n",
    "\n",
    "Since this part enocmpasses some aspects we haven't coverend in the course (yet), some functions have been provided for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we defined a function called `download_book` which downloads a book in .txt format. Then we define a dictionary with names and urls. We loop through the dictionary, download each book and write it to a file stored in the directory `books` in the current working directory. You don't need to do anything - just run the cell and the files will be downloaded to your computer. If it doesn't work, you can download the books manually using the urls below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading data - you get this for free :-) \n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def download_book(url):\n",
    "    \"\"\"\n",
    "    Download book given a url to a book in .txt format and return it as a string.\n",
    "    \"\"\"\n",
    "    text_request = requests.get(url)\n",
    "    text = text_request.text\n",
    "    return text\n",
    "\n",
    "\n",
    "book_urls = dict()\n",
    "book_urls['HuckFinn'] = 'http://www.gutenberg.org/cache/epub/7100/pg7100.txt'\n",
    "book_urls['Macbeth'] = 'http://www.gutenberg.org/cache/epub/1533/pg1533.txt'\n",
    "book_urls['AnnaKarenina'] = 'http://www.gutenberg.org/files/1399/1399-0.txt'\n",
    "\n",
    "\n",
    "if not os.path.isdir('../Data/books/'):\n",
    "    os.mkdir('../Data/books/')\n",
    "    \n",
    "    \n",
    "for name, url in book_urls.items():\n",
    "    text = download_book(url)\n",
    "    with open('../Data/books/'+name+'.txt', 'w') as outfile:\n",
    "        outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Was the download successful? Use a specific python module to print a list of all the files in the directory `../Data/books`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "For testing code, it is always good to use a little toy example. Since we don't want to test our code on all three novels, let's use a single chapter of the Huck Finn novels. Let's say we want to look at Chapter 1 for now. \n",
    "\n",
    "To do this, we need to:\n",
    "\n",
    "* read the correct file\n",
    "* look at the txt file and see how the chapters are separated from each other\n",
    "* assign Chapter 1 to a variable so we can reuse it\n",
    "\n",
    "Solve this first task by writing a function called `get_chapter_HcckFinn` which returns the chapter. Define the function in a way that the user can specify which chapter should be returned. Set the default to Chapter 1.  Use as many helper functions as you need and give them indicative names. \n",
    "\n",
    "Hint: If you want to, take a peek at the next exercises so you can write the functions in a way that you can reuse them (if not, you will have to modify them later). It is always good to plan your code in this way. \n",
    "\n",
    "Please assign Chapter 1 of HuckFinn to a variable called `test_chapter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "3.a) Let's get a little bit of an overview of what we can find in a chapter. Write a function taking a string as input  and printing:\n",
    "\n",
    "* The number of sentences\n",
    "* The number of tokens\n",
    "* The size of the vocabulary used (i.e. unique tokens)\n",
    "\n",
    "Test your function using Chapter 1 of HuckFinn as a test string. \n",
    "\n",
    "3.b) Write a modified version of the function you used to load a single chapter so that it returns all chapters. Attention: The part before the first chapter should not be included. Then iterate over the chapters and print the basic statistics collected in 3.a for each chapter. Don't forget to print the chapter number as well. \n",
    "\n",
    "Hint: If you did not manage 3.a, try to still iterate over the list of chapters and print the chapter number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example solution 3.a\n",
    "\n",
    "import nltk\n",
    "\n",
    "def get_text_stats(text):\n",
    "    \"\"\"\n",
    "    Print number of tokens, sentences and the size of the vocabulary given and input text\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    vocabulary = set(tokens)\n",
    "    \n",
    "    print('Number of tokens:', len(tokens))\n",
    "    print('Number of sentences:', len(sentences))\n",
    "    print('Size of the vocabulary:', len(vocabulary))\n",
    "    \n",
    "get_text_stats(test_chapter)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Time to scale up: Now let's compare the three books to each other. \n",
    "\n",
    "Preparation: Write a function called `get_book` which takes a filepath as input and returns the entire book as text (you have already written this code above, just turn it into an individual function in case you haven't done so already). \n",
    "\n",
    "4.a) Modify your function called `get_chapters` which loads the book as a list of chapters or acts in a way that:\n",
    "\n",
    "* it takes the book as a string as input \n",
    "* it splits it into chapters or acts (look at the way that chapters or acts are separated from each other in the other books and use conditions to split them correctly) and returns a list of chapters or acts\n",
    "\n",
    "Hint 1: You can use the title of the book as a parameter. \n",
    "\n",
    "\n",
    "\n",
    "4.b) Write a function called `get_stats_books` which takes a list of filepaths to the three books as input (we have already created this list above) and prints:\n",
    "\n",
    "* The number of chapters/acts\n",
    "* The total number of sentences in the book\n",
    "* The total number of tokens in the book\n",
    "* The size of the vocabulary used in the book \n",
    "* Don't forget to print the name of each book and leave an empty line in between books\n",
    "\n",
    "Hint 1: You can get the booktitle from the filepath by using the os module and/or applying some string manipulations to the filepath. \n",
    "Hint 2: Use the function you already define above for the statistics (except the number of chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "The statistics above already prove some insights, but we want to know a bit more about what the books are about. To do this, we want to get the 20 most frequent nouns of each book. Since some of us are linguists, we are not satisfied with just looking at the surface forms of the tokens. Instead, we want to lemmatize them first. In addition, we want to write our res\n",
    "ults to files, so we can analyze them later. Since we're getting really good at programming, we are going to move our functions to a python script which we can run from the terminal. Don't worry, we will do everything step by step. \n",
    "\n",
    "\n",
    "Preparation:\n",
    "\n",
    "Write a function which takes text as input and returns a list of lemmas. Hint: We have already written a function called `lemmatize_nouns` above, feel free to use it!\n",
    "\n",
    "5.a.) \n",
    "\n",
    "Write a new function called `count_lemmatized_nouns` which takes text as input and counts the lemmas and returns a list of the most frequent ones. Add a parameter so the user can specify how many should be returned. By default, it should return 20. \n",
    "\n",
    "* Hint 1: Feel free to modify the `lemmatize_nouns` function.\n",
    "\n",
    "* Hint 2: Feel free to use a Counter dictionary (in the collections module - google how to import and use if). You can also build your own dictionary for counting as we did in the Chapter on dictionaries. \n",
    "\n",
    "* Hint 3: Getting the most frequent nouns is really easy with the Counter dictionary in the collections module. If you are not using it, have a look at the `sorted` function and how you can use it on tuples. \n",
    "\n",
    "Test your function using the `test_text` and the `test_chapter` defined above\n",
    "\n",
    "\n",
    "5.b.) \n",
    "\n",
    "Write a function called `lemmas_to_file`. It should take a list of words as input and write them to a file. Each noun should be appear on a new line. The name of the file should be supplied by the user. Test your function using the test_text or test_chapter and call the file `test_chapter_most_frequent_nouns.txt` in your working directory. \n",
    "\n",
    "Hint 1: If you use a context manager, you can loop through a list of lemmas within the manager. \n",
    "\n",
    "Hint 2: To write new lines, remember to simply add the new line character to the string\n",
    "\n",
    "\n",
    "5.c) \n",
    "\n",
    "Time to combine everything! Write a function called `get_nouns_books` which extracts the most frequent lemmas from the three books  in our books/ directory and write them to a files given the filepaths. Make sure the filenames of the new files follow the this convention: `[name]-topnouns20.txt` (e.g. `HuckFinn-topnouns20.txt`). \n",
    "\n",
    "\n",
    "5.d) \n",
    "\n",
    "Now move all your functions to a python script called `analyze_books.py` and call it from the terminal. Don't forget to import all the modules you are using and defining a list of all the filepaths. Don't worry if it runs for a while - it has to process a lot of text! If you computer can't handle it, it is fine if you remove a book from the list (Anna Karenina is by far the longest...). \n",
    "\n",
    "\n",
    "Congratulations! You have written your first little nlp program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

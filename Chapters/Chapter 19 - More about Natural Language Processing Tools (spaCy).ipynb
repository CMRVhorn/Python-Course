{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18 - More about Natural Language Processing Tools (spaCy)\n",
    "\n",
    "Text data is unstructured. But if you want to extract information from text, then you often need to process that data into a more structured representation. The common idea for all Natural Language Processing (NLP) tools is that they try to structure or transform text in some meaningful way. In Chapter 15, you have already learned about four basic NLP steps: sentence splitting, tokenization, POS-tagging and lemmatization. In Assignment 3, you also saw a sentiment analysis tool (the VADER-tool). For all of these, we have used the NLTK library, which is widely used in the field of NLP. However, there are some competitors out there that are worthwhile to have a look at. One of them is spaCy, which is fast and accurate and supports multiple languages. \n",
    "\n",
    "**At the end of this chapter, you will be able to:**\n",
    "- work with spaCy\n",
    "- find some additional NLP tools\n",
    "\n",
    "**If you want to learn more about these topics, you might find the following links useful:**\n",
    "- [5 Heroic Python NLP Libraries](https://elitedatascience.com/python-nlp-libraries)\n",
    "- [NLTK vs. spaCy](https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The NLP pipeline\n",
    "As we already learned in Chapter 16, we often put NLP tasks in a sequence, because they depend on each other. For instance, we need to first tokenize the text (split it into words) in order to be able to assign part-of-speech to each word. This sequence is often called an NLP pipeline. They're called pipelines because they are constructed out\n",
    "of several different *modules*. These are the most common ones:\n",
    "\n",
    "* A tokenizer, to split text into paragraphs, sentences, and words.\n",
    "* A part-of-speech (POS) tagger, to identify different parts of speech (verbs, nouns, ...).\n",
    "* A lemmatizer, to identify word forms with their lemmas (basic word form).\n",
    "* A Named Entity Recognizer (NER), to identify people, locations, organizations, etc.\n",
    "* A dependency parser, to understand the sentence structure.\n",
    "* A semantic role labeler (SRL), to determine who does what to whom (and how).\n",
    "* A word sense disambiguation (WSD) system, to assign the correct meaning to every word.\n",
    "* A polarity tagger, to know whether a sentence is positive or negative.\n",
    "\n",
    "You don't always need all these modules. But it's important to know that they are\n",
    "there, so that you can use them when the need arises.\n",
    "\n",
    "### 1.1 How can you use these modules?\n",
    "\n",
    "Let's be clear about this: **you don't always need to use Python for this**. There are\n",
    "some very strong NLP programs out there that don't rely on Python. You can typically\n",
    "call these programs from the command line. Examples are:\n",
    "\n",
    "* [Treetagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) is a POS-tagger\n",
    "  and lemmatizer in one. It provides support for many different languages. If you want to\n",
    "  call Treetagger from Python, use [treetaggerwrapper](http://treetaggerwrapper.readthedocs.io/).\n",
    "  [Treetagger-python](https://github.com/miotto/treetagger-python) also works, but is much slower.\n",
    "\n",
    "* [Stanford's CoreNLP](http://stanfordnlp.github.io/CoreNLP/) is a very powerful system\n",
    "  that is able to process English, German, Spanish, French, Chinese and Arabic. (Each to\n",
    "  a different extent, though. The pipeline for English is most complete.) There are also\n",
    "  Python wrappers available, such as [py-corenlp](https://github.com/smilli/py-corenlp).\n",
    "\n",
    "* [The Maltparser](http://www.maltparser.org/) has models for English, Swedish, French, and Spanish.\n",
    "\n",
    "And there are many more out there. You'll hear all about these libraries in the NLP toolkits course.\n",
    "Sometimes it's best to just run one of these programs and only analyze the output in Python.\n",
    "\n",
    "Having said that, there are many **NLP-tools that have been developed for Python**:\n",
    "\n",
    "* [Natural Language ToolKit (NLTK)](http://www.nltk.org/): Incredibly versatile library with a bit of everything.\n",
    "  The only downside is that it's not the fastest library out there, and it lags behind the\n",
    "  state-of-the-art.\n",
    "    * Access to several corpora.\n",
    "    * Create a POS-tagger. (Some of these are actually state-of-the-art if you have enough training data.)\n",
    "    * Perform corpus analyses.\n",
    "    * Interface with [WordNet](https://wordnet.princeton.edu/).\n",
    "* [Pattern](http://www.clips.ua.ac.be/pattern): A module that describes itself as a 'web mining module'. Implements a\n",
    "    tokenizer, tagger, parser, and sentiment analyzer for multiple different languages.\n",
    "    Also provides an API for Google, Twitter, Wikipedia and Bing.\n",
    "* [Textblob](http://textblob.readthedocs.io/en/dev/): Another general NLP library that builds on the NLTK and Pattern.\n",
    "* [SpaCy](https://spacy.io/): Tokenizer, POS-tagger, parser and named entity recogniser for English, German, Spanish, Portugese, French, Italian and Dutch (more languages in progress). It can also predict similarity using word embeddings.\n",
    "* [Gensim](http://radimrehurek.com/gensim/): For building vector spaces and topic models.\n",
    "* [Corpkit](http://corpkit.readthedocs.io/en/latest/) is a module for corpus building and corpus management. Includes an interface to the Stanford CoreNLP parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a small NLP pipeline: it takes a raw document, tokenizes it, tags all the tokens, and parses each sentence. On top of that, it also recognizes different types of entities: numbers, locations, and persons. It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of SpaCy is that it is really fast, and it has a good accuracy. In addition, it currently supports multiple languages: English, German, Spanish, Portugese, French, Italian and Dutch. More languages are in progress.\n",
    "\n",
    "In this notebook, we will show you the basic usage. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides good user guides. \n",
    "\n",
    "### 2.1 Installing and loading spaCy\n",
    "\n",
    "To install spaCy, enter the following commands on the command line:\n",
    "\n",
    "* `conda install -c conda-forge spacy`\n",
    "* `python -m spacy download en`\n",
    "\n",
    "If this doesn't work or if you want to download other language models, check out the instructions [here](https://spacy.io/usage/#section-quickstart).\n",
    "\n",
    "Now, let's first load spaCy. We import the spaCy module and load the English tokenizer, tagger, parser, NER and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # other languages: de, es, pt, fr, it, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp` is now a Python object representing the English NLP pipeline that we can use to process a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using spaCy\n",
    "\n",
    "Parsing a text with spaCy after loading a language model is as easy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span objects\n",
    "\n",
    "At this point, there are three important types of objects to remember:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token â€” i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that even though these look like strings, they are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Token` objects have many useful methods (or attributes). As we know by now, we can inspect these methods by using `dir()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a selection of linguistic annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print attributes of tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the difference between token.pos\\_ and token.tag\\_? Read [the docs](https://spacy.io/api/annotation#pos-tagging) to find out.\n",
    "\n",
    "**Question:** what do the different tags mean? Read [this page](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use spacy.explain to find out more about certain labels\n",
    "spacy.explain(\"VBP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `Span` object from the slice doc[start : end]. For instance, doc[2:5] produces a span consisting of tokens 2, 3 and 4. Stepped slices (e.g. doc[start : end : step]) are not supported, as `Span` objects must be contiguous (cannot have gaps). You can use negative indices and open-ended ranges, which have their normal Python semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Span\n",
    "a_slice = doc[2:5]\n",
    "print(a_slice, type(a_slice))\n",
    "\n",
    "# Iterate over Span\n",
    "for token in a_slice:\n",
    "    print(token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text, sentences and noun_chunks\n",
    "\n",
    "If you call the `dir()` function on a `Doc` object, you will see that it has a range of methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we highlight three of them: `text`, `sents` and `noun_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the whole document as a string\n",
    "print(doc.text)\n",
    "print(type(doc.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the sentences as a generator \n",
    "print(doc.sents, type(doc.sents))\n",
    "\n",
    "# You can loop over a generator; each sentence is a span of tokens\n",
    "for sentence in doc.sents:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also store the sentences in a list and then loop over the list \n",
    "sentences = list(doc.sents)\n",
    "for sentence in sentences:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the tokens in the second sentence.\n",
    "sentences = list(doc.sents)\n",
    "for token in sentences[1]:\n",
    "    data = '\\t'.join([token.orth_,\n",
    "                      token.lemma_,\n",
    "                      token.pos_,\n",
    "                      token.tag_,\n",
    "                      str(token.i),    # Turn index into string\n",
    "                      str(token.idx)]) # Turn index into string\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the noun chunks as a generator \n",
    "print(doc.noun_chunks, type(doc.noun_chunks))\n",
    "\n",
    "# You can loop over a generator; each noun chunk is a span of tokens\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk, type(chunk))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a slightly longer text, from the Wikipedia page about Harry Potter.\n",
    "harry_potter = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
    "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
    "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
    "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
    "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
    "\n",
    "sentences = nlp(harry_potter)\n",
    "for e in sentences.ents:\n",
    "    first_word = list(e)[0]\n",
    "    etype = first_word.ent_type_\n",
    "    print(e,'\\t',etype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, but what does NORP mean? You can use spacy.explain() to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK versus spaCy\n",
    "\n",
    "There a difference in quality between spaCy and the NLTK, with the former being superior. But how can you tell? Here's an example of both tools in action. \n",
    "\n",
    "* The example text is a case in point. What goes wrong here?\n",
    "* Try experimenting with the text to see what the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only load this cell if you haven't loaded SpaCy and nltk yet. E.g. if you restarted this notebook.\n",
    "import nltk\n",
    "from spacy.en import English\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like cheese very much\"\n",
    "\n",
    "print(\"NLTK results:\")\n",
    "nltk_tagged = nltk.pos_tag(text.split())\n",
    "print(nltk_tagged)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"spaCy results:\")\n",
    "doc = nlp(text)\n",
    "spacy_tagged = []\n",
    "for token in doc:\n",
    "    tag_data = (token.orth_, token.tag_,)\n",
    "    spacy_tagged.append(tag_data)\n",
    "print(spacy_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Some other useful modules for cleaning and preprocessing\n",
    "\n",
    "Data is often messy, noisy or includes irrelevant information. Therefore, chances are big that you will need to do some cleaning before you can start with your analysis. This is especially true for social media texts, such as tweets, chats, and emails. Typically, these texts are informal and notoriously noisy. Normalising them to be able to process them with NLP tools is a NLP challenge in itself and fully discussing it goes beyond the scope of this course. However, you may find the following modules useful in your project:\n",
    "\n",
    "- [tweet-preprocessor](https://pypi.python.org/pypi/tweet-preprocessor/0.4.0): This library makes it easy to clean, parse or tokenize the tweets. It supports cleaning, tokenizing and parsing of URLs, hashtags, reserved words, mentions, emojis and smileys.\n",
    "- [emot](https://pypi.python.org/pypi/emot/1.0): Emot is a python library to extract the emojis and emoticons from a text (string). All the emojis and emoticons are taken from a reliable source, i.e. Wikipedia.org.\n",
    "- [autocorrect](https://pypi.python.org/pypi/autocorrect/0.1.0): Spelling corrector (Python 3).\n",
    "- [html](https://docs.python.org/3/library/html.html#module-html): Can be used to remove HTML tags.\n",
    "- [chardet](https://pypi.python.org/pypi/chardet): Universal encoding detector for Python 2 and 3.\n",
    "- [ftfy](https://pypi.python.org/pypi/ftfy): Fixes broken unicode strings.\n",
    "\n",
    "If you are interested in reading more about these topic, these papers discuss preprocessing and normalization:\n",
    "\n",
    "* [Assessing the Consequences of Text Preprocessing Decisions](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2849145) (Denny & Spirling 2016). This paper is a bit long, but it provides a nice discussion of common preprocessing steps and their potential effects.\n",
    "* [What to do about bad language on the internet](http://www.cc.gatech.edu/~jeisenst/papers/naacl2013-badlanguage.pdf) (Eisenstein 2013). This is a quick read that I recommend everyone to at least look through.\n",
    "\n",
    "And [here](https://www.kaggle.com/rtatman/character-encodings-tips-tricks/) is a nice blog about character encoding."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

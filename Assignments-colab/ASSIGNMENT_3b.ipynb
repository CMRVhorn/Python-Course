{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ASSIGNMENT-3b.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cltl/python-for-text-analysis/blob/colab/Assignments-colab/ASSIGNMENT_3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpcaZLBYNzmm"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/Data.zip\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/images.zip\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/Extra_Material.zip\n",
        "\n",
        "!unzip Data.zip -d ../\n",
        "!unzip images.zip -d ./\n",
        "!unzip Extra_Material.zip -d ../\n",
        "\n",
        "!rm Data.zip\n",
        "!rm Extra_Material.zip\n",
        "!rm images.zip\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9arCT4eNzLN"
      },
      "source": [
        "# Assignment 3b: Writing your own nlp program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9-U6uxNzLR"
      },
      "source": [
        "## Due: Friday the 1st of October 2021 14:30.\n",
        "\n",
        "* Please submit your assignment (notebooks of parts 3a and 3b + additional files) as **a single .zip file** using Canvas (Assignments --> Assignment 3)\n",
        "\n",
        "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
        "\n",
        "**IMPORTANTE NOTE**:\n",
        "* The students who follow the Bachelor version of this course, i.e., the course Introduction to Python for Humanities and Social Sciences (L_AABAALG075) as part of the minor Digital Humanities, do **not have to do Exercises 3 and 4 of Assignment 3b**\n",
        "* The other students, i.e., who follow the Master version of  course, which is Programming in Python for Text Analysis (L_AAMPLIN021), are required to **do Exercises 3 and 4 of Assignment 3b**\n",
        "\n",
        "If you have **questions** about this topic, please contact us **(cltl.python.course@gmail.com)**. Questions and answers will be collected on Piazza, so please check if your question has already been answered first.\n",
        "\n",
        "In this part of the assignment, we will carry out our own little text analysis project. The goal is to gain some insights into longer texts without having to read them all in detail. \n",
        "\n",
        "This part of the assignment builds on some notions that have been revised in part A of the assignment. Please feel free to go back to part A and reuse your code whenever possible. \n",
        "\n",
        "The goals of this part are:\n",
        "\n",
        "* divide a problem into smaller sub-problems and test code using small examples\n",
        "* doing text analysis and writing results to a file\n",
        "* combining small functions into bigger functions\n",
        "\n",
        "**Tip**: The assignment is split into four steps, which are divided into smaller steps. Instead of doing everything step by step, we highly recommend you read all sub-steps of a step first and then start coding. In many cases, the sub-steps are there to help you split the problem into manageable sub-problems, but it is still good to keep the overall goal in mind. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKtpEcDSNzLR"
      },
      "source": [
        "## Preparation: Data collection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZ8_TgANzLS"
      },
      "source": [
        "In the directory `../Data/books/`, you should find three .txt files. If not, you can use the following cell to download them. Also, feel free to look at the code to learn how to download .txt files from the web. \n",
        "\n",
        "We defined a function called `download_book` which downloads a book in .txt format. Then we define a dictionary with names and urls. We loop through the dictionary, download each book and write it to a file stored in the directory `books` in the current working directory. You don't need to do anything - just run the cell and the files will be downloaded to your computer.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ty6_dYrNzLS"
      },
      "source": [
        "# Downloading data - you get this for free :-) \n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "\n",
        "def download_book(url):\n",
        "    \"\"\"\n",
        "    Download book given a url to a book in .txt format and return it as a string.\n",
        "    \"\"\"\n",
        "    text_request = requests.get(url)\n",
        "    text = text_request.text\n",
        "    return text\n",
        "\n",
        "\n",
        "book_urls = dict()\n",
        "book_urls['HuckFinn'] = 'http://www.gutenberg.org/cache/epub/7100/pg7100.txt'\n",
        "book_urls['Macbeth'] = 'http://www.gutenberg.org/cache/epub/1533/pg1533.txt'\n",
        "book_urls['AnnaKarenina'] = 'http://www.gutenberg.org/files/1399/1399-0.txt'\n",
        "\n",
        "\n",
        "if not os.path.isdir('../Data/books/'):\n",
        "    os.mkdir('../Data/books/')\n",
        "    \n",
        "    \n",
        "for name, url in book_urls.items():\n",
        "    text = download_book(url)\n",
        "    with open('../Data/books/'+name+'.txt', 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZlucALNzLU"
      },
      "source": [
        "**Encoding issues with txt files**\n",
        "\n",
        "For Windows users, the file “AnnaKarenina.txt” gets the encoding cp1252. \n",
        "In order to open the file, you have to add **encoding='utf-8'**, i.e.,\n",
        "\n",
        "```python\n",
        "a_path = 'some path on your computer.txt'\n",
        "with open(a_path, mode='r', encoding='utf-8'):\n",
        "    # process file\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbWM1JApNzLV"
      },
      "source": [
        "## Exercise 1\n",
        "Was the download successful? Let's start writing code! Please create the following two Python modules:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf1abi5INzLV"
      },
      "source": [
        "* Python module **analyze.py** This module you will call from the command line\n",
        "* Python module **utils.py** This module will contain your helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkwO46DvNzLW"
      },
      "source": [
        "More precisely, please create two files, **analyze.py** and **utils.py**, which are both placed in the same directory as this notebook. The two files are empty at this stage of the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgaXHbVLNzLW"
      },
      "source": [
        "1.a) Write a function called `get_paths` and store it in the Python module **utils.py**\n",
        "\n",
        "The function `get_paths`:\n",
        "* takes one positional parameter called *input_folder*\n",
        "* the function stores all paths to .txt files in the *input_folder* in a list\n",
        "* the function returns a list of strings, i.e., each string is a file path\n",
        "\n",
        "Once you've created the function and stored it in **utils.py**:\n",
        "* Import the function into **analyze.py**, using `from utils import get_paths`\n",
        "* Call the function inside **analyze.py** (input_folder=\"../Data/books\")\n",
        "* Assign the output of the function to a variable and print this variable.\n",
        "* call **analyze.py** from the command line to test it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa9JLcdTNzLW"
      },
      "source": [
        "## Exercise 2\n",
        "2.a) Let's get a little bit of an overview of what we can find in each text. Write a function called `get_basic_stats`.\n",
        "\n",
        "The function `get_basic_stats`:\n",
        "* has one positional parameter called **txt_path** which is the path to a txt file\n",
        "* reads the content of the txt file into a string\n",
        "* Computes the following statistics:\n",
        "    * The number of sentences\n",
        "    * The number of tokens\n",
        "    * The size of the vocabulary used (i.e. unique tokens)\n",
        "    * the number of chapters/acts:\n",
        "        * count occurrences of 'CHAPTER' in **HuckFinn.txt**\n",
        "        * count occurrences of 'Chapter ' (with the space) in **AnnaKarenina.txt**\n",
        "        * count occurrences of 'ACT' in **Macbeth.txt**\n",
        "* return a dictionary with four key:value pairs, one for each statistic described above:\n",
        "    * num_sents\n",
        "    * num_tokens\n",
        "    * vocab_size \n",
        "    * num_chapters_or_acts\n",
        "\n",
        "In order to compute the statistics, you need to perform sentence splitting and tokenization. Here is an example snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Q1EGBEUqNzLX",
        "outputId": "17526944-bb4e-4e79-be2a-890b7b5df83a"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = 'Python is a programming language. It was created by Guido van Rossum.'\n",
        "for sent in sent_tokenize(text):\n",
        "    print('SENTENCE', sent)\n",
        "    tokens = word_tokenize(sent)\n",
        "    print('TOKENS', tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f38d5a177ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Python is a programming language. It was created by Guido van Rossum.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SENTENCE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24IMAc9NzLY"
      },
      "source": [
        "2.b) Store the function in the Python module **utils.py**. Import it in **analyze.py**. \n",
        "Edit **analyze.py** so that:\n",
        "* you first call the function **get_paths**\n",
        "* create an empty dictionary called **book2stats**, i.e., `book2stats = {}`\n",
        "* Loop over the list of txt files (the output from **get_paths**) and call the function **get_basic_stats** on each file\n",
        "* print the output of calling the function **get_basic_stats** on each file.\n",
        "* update the dictionary **book2stats** with each iteration of the for loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VteAgollNzLY"
      },
      "source": [
        "Tip: **book2stats** is a dictionary mapping a book name (the key), e.g., ‘AnnaKarenina’, to a dictionary (the value) (the output from get_basic_stats)\n",
        "\n",
        "Tip: please use the following code snippet to obtain the basename name of a file path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MgO2g9NNzLY",
        "outputId": "11f01c8e-b388-4372-f186-ca743b24c2f4"
      },
      "source": [
        "import os \n",
        "basename = os.path.basename('../Data/books/HuckFinn.txt')\n",
        "book = basename.strip('.txt')\n",
        "print(book)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuckFinn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jERV_TVENzLZ"
      },
      "source": [
        "### Please note that Exercises 3 and 4, respectively, are designed to be difficult. You will have to combine what you have learnt so far to complete these exercises. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p8XFNXtNzLZ"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "\n",
        "Let's compare the books based on the statistics. Create a dictionary `stats2book_with_highest_value` in **analyze.py** with four keys:\n",
        "* num_sents\n",
        "* num_tokens\n",
        "* vocab_size \n",
        "* num_chapters_or_acts\n",
        "\n",
        "The values are not the frequencies, but the **book** that has the highest value for the statistic. Make use of the **book2stats** dictionary to accomplish this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdSB2uJeNzLZ"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "4.a) The statistics above already provide some insights, but we want to know a bit more about what the books are about. To do this, we want to get the 30 most frequent tokens of each book. Edit the function `get_basic_stats` to add one more key:value pair:\n",
        "* the key is **top_30_tokens**\n",
        "* the value is a list of the 30 most frequent words in the text.\n",
        "\n",
        "4.b) Write the top 30 tokens (one on each line) for each file to disk using the naming `top_30_[FILENAME]`:\n",
        "* top_30_AnnaKarenina.txt\n",
        "* top_30_HuckFinn.txt\n",
        "* top_30_Macbeth.txt\n",
        "\n",
        "Example of file (*the* and *and* may not be the most frequent tokens, these are just examples):\n",
        "\n",
        "```\n",
        "the \n",
        "and \n",
        "..\n",
        "```\n",
        "The following code snippet can help you with obtaining the top 30 occurring tokens. The goal is to call the function you updated in Exercise 4a, i.e., get_basic_stats, in the file analyze.py. This also makes it possible to write the top 30 tokens to files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgqjAcVpNzLa",
        "outputId": "4e969e0f-e914-48bf-cc6d-040100e21059"
      },
      "source": [
        "import operator \n",
        "\n",
        "token2freq = {'a': 1000, 'the': 100, 'cow' : 5}\n",
        "for token, freq in sorted(token2freq.items(), \n",
        "                          key=operator.itemgetter(1),\n",
        "                          reverse=True):\n",
        "    print(token, freq)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 1000\n",
            "the 100\n",
            "cow 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzXThUZfNzLa"
      },
      "source": [
        "Congratulations! You have written your first little nlp program!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2VAdGqNzLa"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}